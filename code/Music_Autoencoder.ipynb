{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T23:28:34.284216Z",
     "start_time": "2020-05-09T23:28:34.278168Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from music21 import *\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout,LSTM\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formatting with character to character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T22:54:53.917311Z",
     "start_time": "2020-05-09T22:54:53.914684Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\".../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=open(\"jiggs.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=text.split('\\n\\n')\n",
    "corpus=[song for song in clean_text if song!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X: 1\\nT:A and D\\n% Nottingham Music Database\\nS:EF\\nM:4/4\\nK:A\\nM:6/8\\nP:A\\nf|\"A\"ecc c2f|\"A\"ecc c2f|\"A\"ecc c2f|\"Bm\"BcB \"E7\"B2f|\\n\"A\"ecc c2f|\"A\"ecc c2c/2d/2|\"D\"efe \"E7\"dcB| [1\"A\"Ace a2:|\\n [2\"A\"Ace ag=g||\\\\\\nK:D\\nP:B\\n\"D\"f2f Fdd|\"D\"AFA f2e/2f/2|\"G\"g2g ecd|\"Em\"efd \"A7\"cBA|\\n\"D\"f^ef dcd|\"D\"AFA f=ef|\"G\"gfg \"A7\"ABc |1\"D\"d3 d2e:|2\"D\"d3 d2||'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_sentence=[]\n",
    "suffix_sentence=[]\n",
    "\n",
    "for music in corpus:\n",
    "    for index in range(len(music)):\n",
    "        prefix=music[:index+1]\n",
    "        suffix='\\t'+ music[index+1:]+'\\n'\n",
    "        prefix_sentence.append(prefix)\n",
    "        suffix_sentence.append(suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(['\\t', '\\n'])\n",
    "\n",
    "for music in corpus:\n",
    "    for char in music:\n",
    "        if char not in vocabulary:\n",
    "            vocabulary.add(char)\n",
    "            \n",
    "vocabulary=sorted(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = dict((char, idx) for idx, char in enumerate(vocabulary))\n",
    "idx_to_char = dict((idx, char) for idx, char in enumerate(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the length of the longest prefix\n",
    "max_len_prefix_sent = max([len(prefix) for prefix in prefix_sentence])\n",
    "\n",
    "# Find the length of the longest suffix\n",
    "max_len_suffix_sent = max([len(suffix) for suffix in suffix_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 3-D zero vector for the prefix sentences\n",
    "input_data_prefix = np.zeros((len(prefix_sentence), max_len_prefix_sent, \n",
    "                              len(vocabulary)), dtype='float32')\n",
    "\n",
    "# Define a 3-D zero vector for the suffix sentences\n",
    "input_data_suffix = np.zeros((len(suffix_sentence), max_len_suffix_sent, \n",
    "                              len(vocabulary)), dtype='float32')\n",
    "\n",
    "# Define a 3-D zero vector for the target data\n",
    "target_data = np.zeros((len(suffix_sentence), max_len_suffix_sent, \n",
    "                        len(vocabulary)), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(prefix_sentence)):\n",
    "    # Iterate over each character in each prefix\n",
    "    for k, ch in enumerate(prefix_sentence[i]):\n",
    "        # Convert the character to a one-hot encoded vector\n",
    "        input_data_prefix[i, k, char_to_idx[ch]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate over each character in each suffix\n",
    "for k, ch in enumerate(suffix_sentence[i]):\n",
    "    # Convert the character to a one-hot encoded vector\n",
    "    input_data_suffix[i, k, char_to_idx[ch]] = 1\n",
    "\n",
    "    # Target data is one timestep ahead and excludes start character\n",
    "    if k > 0:\n",
    "        target_data[i, k-1, char_to_idx[ch]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input layer of the encoder\n",
    "encoder_input = Input(shape=(None, len(vocabulary)))\n",
    "\n",
    "# Create LSTM Layer of size 256\n",
    "encoder_LSTM = LSTM(50, return_state = True)\n",
    "\n",
    "# Save encoder output, hidden and cell state\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM(encoder_input)\n",
    "\n",
    "# Save encoder states\n",
    "encoder_states = [encoder_h, encoder_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decoder input layer\n",
    "decoder_input = Input(shape=(None, len(vocabulary)))\n",
    "\n",
    "# Create LSTM layer of size 256\n",
    "decoder_LSTM = LSTM(50,return_sequences=True, return_state = True)\n",
    "\n",
    "# Save decoder output\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "\n",
    "# Create a `Dense` layer with softmax activation\n",
    "decoder_dense = Dense(len(vocabulary),activation='softmax')\n",
    "\n",
    "# Save the decoder output\n",
    "decoder_out = decoder_dense(decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 88)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 88)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50), (None,  27800       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 50), ( 27800       input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 88)     4488        lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 60,088\n",
      "Trainable params: 60,088\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103440 samples, validate on 25860 samples\n",
      "Epoch 1/1\n",
      "103440/103440 [==============================] - 4828s 47ms/step - loss: 0.0000e+00 - val_loss: 1.3337e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2a89356bb0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[input_data_prefix, input_data_suffix], y=target_data,\n",
    "          batch_size=64, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Create decoder input states for inference\n",
    "decoder_state_input_h = Input(shape=(50,))\n",
    "decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Get decoder output and feed it to the dense layer for final output prediction\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, initial_state=decoder_input_states)\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "# Create decoder inference model\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states, outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass input prefix to the Encoder inference model and get the states\n",
    "inp_seq = input_data_prefix[50:100]\n",
    "states_val = encoder_model_inf.predict(inp_seq)\n",
    "\n",
    "# Seed the first character and get output from the decoder \n",
    "target_seq = np.zeros((1, 1, len(vocabulary)))\n",
    "target_seq[0, 0, char_to_idx['\\t']] = 1  \n",
    "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "\n",
    "# Find out the next character from the Decoder output\n",
    "max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "sampled_suffix_char = idx_to_char[max_val_index]\n",
    "\n",
    "# Print the first character\n",
    "print(sampled_suffix_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the generated character from last time to the target sequence \n",
    "target_seq = np.zeros((1, 1, len(vocabulary)))\n",
    "target_seq[0, 0, max_val_index] = 1\n",
    "\n",
    "# Initialize the decoder state to the states from last iteration\n",
    "states_val = [decoder_h, decoder_c]\n",
    "\n",
    "# Get decoder output\n",
    "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "\n",
    "# Get most probable next character and print it.\n",
    "max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "sampled_suffix_char = idx_to_char[max_val_index]\n",
    "print(sampled_suffix_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_suffix_sentence(inp_seq):\n",
    "\n",
    "    # Initialize states value to the final states of the encoder\n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "\n",
    "    # Initialize the target sequence to contain the start token\n",
    "    target_seq = np.zeros((1, 1, len(vocabulary)))\n",
    "    target_seq[0, 0, char_to_idx['\\t']] = 1\n",
    "\n",
    "    # Define a variable to store the suffix sentence\n",
    "    suffix_sent = ''\n",
    "\n",
    "    # Define stop condition flag\n",
    "    stop_condition = False\n",
    "\n",
    "    # Iterate until the end token is found or maximum length of the suffix sentence is reached\n",
    "    while not stop_condition:\n",
    "\n",
    "        # Get output from decoder inference model\n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "\n",
    "        # Get most probable next character\n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_output_char = idx_to_char[max_val_index]\n",
    "\n",
    "        # Append the generated char to the suffix sentence\n",
    "        suffix_sent += sampled_output_char\n",
    "\n",
    "        # Check if end token is encountered or maximum length of the suffix sentence is exceeded\n",
    "        if ((sampled_output_char == '\\n') or (len(suffix_sent) > max_len_suffix_sent)) :\n",
    "            stop_condition = True\n",
    "\n",
    "        # Add the new generated char to the existing target sequence\n",
    "        target_seq = np.zeros((1, 1, len(vocabulary)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "\n",
    "        # Save state values to use in the next iteration\n",
    "        states_val = [decoder_h, decoder_c]\n",
    "\n",
    "    # Return the suffix sentence\n",
    "    return suffix_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate 10 suffixes\n",
    "for seq_index in range(10):\n",
    "  \n",
    "    # Get the next tokenized sentence\n",
    "    inp_seq = input_data_prefix[seq_index:seq_index+1]\n",
    "    \n",
    "    # Generate the suffix sentence\n",
    "    suffix_sent = generate_suffix_sentence(inp_seq)\n",
    "    \n",
    "    # Print the prefix sentence\n",
    "    print('Prefix Sentence:', prefix_sentence[seq_index])\n",
    "    \n",
    "    # Print the suffix sentence\n",
    "    print('Suffix Sentence:', suffix_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "210px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
